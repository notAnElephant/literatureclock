import argparse
import json
import logging
import random
import re
from pathlib import Path
from collections import defaultdict

from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.common.exceptions import TimeoutException, WebDriverException
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait, Select
from webdriver_manager.chrome import ChromeDriverManager

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def load_rules(path):
    """Loads rules.json5, stripping comments."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        content = re.sub(r'//.*', '', content)
        return json.loads(content)
    except Exception as e:
        logging.error(f"Failed to load rules from {path}: {e}")
        return None

class TimeTermGenerator:
    def __init__(self, rules):
        self.rules = rules
        self.number_words = rules.get('number_words', {})
        
    def get_number_word(self, n):
        n_str = str(n)
        words = []
        if n_str in self.number_words:
            return self.number_words[n_str]
        if 13 <= n <= 19:
            prefix = self.number_words.get('13_19_prefix', ['tizen'])[0]
            digit = n % 10
            digit_words = self.get_number_word(digit)
            for dw in digit_words:
                words.append(prefix + dw)
        elif 21 <= n <= 29:
            prefix = self.number_words.get('20s_prefix', ['huszon'])[0]
            digit = n % 10
            digit_words = self.get_number_word(digit)
            for dw in digit_words:
                words.append(prefix + dw)
        elif n == 20:
             return self.number_words.get('20_exact', ['húsz'])
        elif n == 30:
             return self.number_words.get('30_exact', ['harminc'])
        elif 31 <= n <= 39:
            prefix = self.number_words.get('30s_prefix', ['harminc'])[0]
            digit = n % 10
            digit_words = self.get_number_word(digit)
            for dw in digit_words:
                words.append(prefix + dw)
        elif n == 40:
             return self.number_words.get('40_exact', ['negyven'])
        elif 41 <= n <= 49:
            prefix = self.number_words.get('40s_prefix', ['negyven'])[0]
            digit = n % 10
            digit_words = self.get_number_word(digit)
            for dw in digit_words:
                words.append(prefix + dw)
        elif n == 50:
             return self.number_words.get('50_exact', ['ötven'])
        elif 51 <= n <= 59:
            prefix = self.number_words.get('50s_prefix', ['ötven'])[0]
            digit = n % 10
            digit_words = self.get_number_word(digit)
            for dw in digit_words:
                words.append(prefix + dw)
        if not words and n_str in self.number_words:
             return self.number_words[n_str]
        return words if words else [str(n)]

    def generate_terms(self, h, m):
        terms = set()
        # Removed dotted patterns as requested
        terms.add(f"{h}:{m:02}")
        terms.add(f"{h:02}:{m:02}")
        terms.add(f"{h} óra {m} perc")
        terms.add(f"{h:02} óra {m:02} perc")
        terms.add(f"{h} óra {m:02} perc")
        
        h_words = self.get_number_word(h)
        m_words = self.get_number_word(m)
        
        for hw in h_words:
            for mw in m_words:
                terms.add(f"{hw} óra {mw} perc")
            terms.add(f"{hw} óra {m} perc")
            terms.add(f"{hw} óra {m:02} perc")

        if m == 0:
            terms.add(f"{h} óra")
            terms.add(f"{h:02} óra")
            terms.add(f"{h} órakor")
            terms.add(f"{h}-kor")
            for hw in h_words:
                terms.add(f"{hw} óra")
                terms.add(f"{hw} órakor")
                terms.add(f"{hw}-kor") 

        next_h = (h + 1) % 24
        next_h_words = self.get_number_word(next_h)
        
        if m == 30:
            terms.add(f"fél {next_h}")
            for w in next_h_words:
                terms.add(f"fél {w}")
        
        if m == 15:
            terms.add(f"negyed {next_h}")
            for w in next_h_words:
                terms.add(f"negyed {w}")

        if m == 45:
            terms.add(f"háromnegyed {next_h}")
            for w in next_h_words:
                terms.add(f"háromnegyed {w}")

        if m > 0:
             terms.add(f"{m} perccel {h} óra után")
             for mw in m_words:
                 terms.add(f"{mw} perccel {h} óra után")
                 for hw in h_words:
                      terms.add(f"{mw} perccel {hw} óra után")
        
        y_before = 60 - m
        if 0 < y_before < 60:
             target_next_h = (h + 1) % 24
             terms.add(f"{y_before} perccel {target_next_h} óra előtt")
             y_before_words = self.get_number_word(y_before)
             target_next_h_words = self.get_number_word(target_next_h)
             for ybw in y_before_words:
                 terms.add(f"{ybw} perccel {target_next_h} óra előtt")
                 for tnhw in target_next_h_words:
                      terms.add(f"{ybw} perccel {tnhw} óra előtt")

        return list(terms)

class MekSearcher:
    def __init__(self, headless=True):
        self.headless = headless
        self.options = webdriver.ChromeOptions()
        if headless:
            self.options.add_argument("--headless")
        self.options.add_argument("--no-sandbox")
        self.options.add_argument("--disable-dev-shm-usage")
        self._init_driver()
        self.url = "https://mek.oszk.hu/hu/search/elfulltext/#sealist"

    def _init_driver(self):
        logging.info("Initializing Chrome Driver...")
        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=self.options)

    def restart_driver(self):
        logging.warning("Restarting Chrome Driver due to error...")
        try:
            self.driver.quit()
        except Exception:
            pass
        self._init_driver()
        
    def search(self, term):
        # Retry loop for driver stability
        for attempt in range(2):
            try:
                return self._search_attempt(term)
            except WebDriverException as e:
                logging.error(f"WebDriver error during search for '{term}' (attempt {attempt+1}/2): {e}")
                if attempt == 0:
                    self.restart_driver()
                else:
                    logging.error("Failed to search even after restart.")
                    return []
            except Exception as e:
                logging.error(f"Unexpected error during search for '{term}': {e}")
                return []
        return []

    def _search_attempt(self, term):
        raw_results = []
        logging.info(f"Navigating to {self.url}...")
        self.driver.get(self.url)
        search_input = WebDriverWait(self.driver, 10).until(
            EC.presence_of_element_located((By.NAME, "body"))
        )
        
        # Set results per page to 100
        try:
            size_select = Select(self.driver.find_element(By.NAME, "size"))
            size_select.select_by_value("100")
        except Exception as e:
            logging.warning(f"Could not set result size to 100: {e}")

        quoted_term = f'"{term}"'
        logging.info(f"Searching for: {quoted_term}")
        search_input.clear()
        search_input.send_keys(quoted_term)
        submit_btn = self.driver.find_element(By.XPATH, "//input[@type='submit']")
        submit_btn.click()
        
        # Wait for results
        try:
            WebDriverWait(self.driver, 5).until(
                EC.presence_of_element_located((By.CLASS_NAME, "hit"))
            )
        except TimeoutException:
            logging.info("  -> No hits found (timeout waiting for .hit).")
            return []
        
        # Grab all HTML immediately
        hit_divs = self.driver.find_elements(By.CLASS_NAME, "hit")
        logging.info(f"Found {len(hit_divs)} hit blocks.")
        
        hits_html = []
        for div in hit_divs:
            try:
                hits_html.append(div.get_attribute('outerHTML'))
            except Exception as e:
                logging.warning(f"Error grabbing HTML for a hit: {e}")

        # Parse with BeautifulSoup
        for i, html in enumerate(hits_html):
            try:
                soup = BeautifulSoup(html, 'html.parser')
                
                link_elem = soup.find('a', class_='etitem')
                if not link_elem:
                    logging.warning(f"Hit {i}: Could not find .etitem inside .hit")
                    continue

                link = link_elem.get('href', '')
                
                author_elem = link_elem.find(class_='dcauthor')
                author = author_elem.get_text(strip=True) if author_elem else ""
                    
                title_elem = link_elem.find(class_='dctitle')
                title = title_elem.get_text(strip=True) if title_elem else ""
                    
                snippet_elem = link_elem.find(class_='foundtext')
                snippet = str(snippet_elem) if snippet_elem else ""
                
                full_title = f"{author}: {title}" if author else title

                if full_title:
                    raw_results.append({
                        "search_term": term,
                        "title": full_title,
                        "link": link,
                        "snippet": snippet
                    })
                else:
                        logging.warning(f"Hit {i}: Skipped because title is empty.")

            except Exception as e:
                logging.warning(f"Error parsing hit block {i}: {e}")
        
        # Filter results based on literature category
        if raw_results:
            logging.info(f"Checking {len(raw_results)} hits for literature category...")
            valid_results = []
            fallback_results = []
            
            for res in raw_results:
                is_lit, topics = self.check_is_literature(res['link'])
                res['is_literature'] = is_lit
                res['topics'] = topics
                if is_lit:
                    valid_results.append(res)
                else:
                    fallback_results.append(res)
            
            if valid_results:
                logging.info(f"  -> {len(valid_results)} literature hits kept.")
                return valid_results
            elif fallback_results:
                logging.info(f"  -> 0 literature hits. Returning {len(fallback_results)} non-literature hits as fallback.")
                return fallback_results
        
        return []

    def check_is_literature(self, link):
        if not link:
            return False, []
        try:
            self.driver.get(link)
            tags = self.driver.find_elements(By.CSS_SELECTOR, ".topic, .subtopic")
            topics = [t.text for t in tags]
            is_lit = any("irodalom" in t.lower() for t in topics)
            return is_lit, topics
        except WebDriverException:
            # Re-raise WebDriverException to trigger driver restart in search()
            raise
        except Exception as e:
            logging.warning(f"Failed to check link {link}: {e}")
            return False, []

    def close(self):
        self.driver.quit()

def main():
    parser = argparse.ArgumentParser(description="Search MEK for time patterns.")
    parser.add_argument("--limit", type=int, default=5, help="Max number of terms to search.")
    parser.add_argument("--output", default="mek_search_results.jsonl", help="Output file path.")
    parser.add_argument("--visible", action="store_true", help="Run browser in visible mode.")
    parser.add_argument("--term", help="Search for a specific term (ignores generator).")
    args = parser.parse_args()

    rules_path = Path(__file__).parent.parent.parent / 'rules.json5'
    rules = load_rules(rules_path)
    if not rules:
        logging.error("Could not load rules. Exiting.")
        return

    searcher = MekSearcher(headless=not args.visible)

    try:
        term_to_times = defaultdict(set)
        
        if args.term:
            term = args.term
            logging.info(f"Single term mode: {term}")
            search_queue = [term]
        else:
            generator = TimeTermGenerator(rules)
            logging.info("Generating search terms...")
            for h in range(24):
                for m in range(60):
                    terms = generator.generate_terms(h, m)
                    time_str = f"{h:02}:{m:02}"
                    for t in terms:
                        term_to_times[t].add(time_str)
            
            sorted_terms = sorted(list(term_to_times.keys()))
            logging.info(f"Generated {len(sorted_terms)} unique search terms.")
            
            if args.limit > 0:
                logging.info(f"Test mode: selecting {args.limit} random terms.")
                search_queue = random.sample(sorted_terms, min(args.limit, len(sorted_terms)))
            else:
                search_queue = sorted_terms

        logging.info(f"Starting search for {len(search_queue)} terms...")
        
        with open(args.output, 'w', encoding='utf-8') as f:
            for i, term in enumerate(search_queue):
                logging.info(f"[{i+1}/{len(search_queue)}] Searching: {term}")
                results = searcher.search(term)
                
                valid_times = list(term_to_times.get(term, []))
                
                if results:
                    logging.info(f"  -> Found {len(results)} matches.")
                    for res in results:
                        res["valid_times"] = valid_times
                        f.write(json.dumps(res, ensure_ascii=False) + "\n")
                else:
                    logging.info("  -> No matches.")
                    no_match_record = {
                        "search_term": term,
                        "valid_times": valid_times,
                        "count": 0
                    }
                    f.write(json.dumps(no_match_record, ensure_ascii=False) + "\n")
                f.flush()

    finally:
        searcher.close()

if __name__ == "__main__":
    main()
